{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 30:\n",
      "Image - Min Value: 7 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTNADQGOCHgAaE/QA0Nj/Ag23tLCeZ2LvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37e4709198>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 30\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[101 208 145]\n",
      "  [  4 156 226]\n",
      "  [141 126  75]\n",
      "  ..., \n",
      "  [133  62  16]\n",
      "  [ 14  24  26]\n",
      "  [116  81 149]]\n",
      "\n",
      " [[162 147 107]\n",
      "  [  9  17 133]\n",
      "  [149 171 216]\n",
      "  ..., \n",
      "  [ 45 204 166]\n",
      "  [ 89 166 144]\n",
      "  [115  61 235]]\n",
      "\n",
      " [[ 22 179 197]\n",
      "  [206 101 135]\n",
      "  [ 37  97 233]\n",
      "  ..., \n",
      "  [215 193  69]\n",
      "  [112  81 160]\n",
      "  [ 21  99  97]]\n",
      "\n",
      " ..., \n",
      " [[  1  89 233]\n",
      "  [ 77  65 126]\n",
      "  [ 88  27  47]\n",
      "  ..., \n",
      "  [138  56 131]\n",
      "  [215 254 127]\n",
      "  [183  15  22]]\n",
      "\n",
      " [[228 177 212]\n",
      "  [118 133  21]\n",
      "  [178  52  83]\n",
      "  ..., \n",
      "  [167 109  27]\n",
      "  [168 156 251]\n",
      "  [117 175  75]]\n",
      "\n",
      " [[ 35  92 131]\n",
      "  [  2   6 126]\n",
      "  [225  79  16]\n",
      "  ..., \n",
      "  [111  79 159]\n",
      "  [228  88 127]\n",
      "  [ 30  58 201]]]\n",
      "707\n",
      "[[[ 0.39607843  0.81568627  0.56862745]\n",
      "  [ 0.01568627  0.61176471  0.88627451]\n",
      "  [ 0.55294118  0.49411765  0.29411765]\n",
      "  ..., \n",
      "  [ 0.52156863  0.24313725  0.0627451 ]\n",
      "  [ 0.05490196  0.09411765  0.10196078]\n",
      "  [ 0.45490196  0.31764706  0.58431373]]\n",
      "\n",
      " [[ 0.63529412  0.57647059  0.41960784]\n",
      "  [ 0.03529412  0.06666667  0.52156863]\n",
      "  [ 0.58431373  0.67058824  0.84705882]\n",
      "  ..., \n",
      "  [ 0.17647059  0.8         0.65098039]\n",
      "  [ 0.34901961  0.65098039  0.56470588]\n",
      "  [ 0.45098039  0.23921569  0.92156863]]\n",
      "\n",
      " [[ 0.08627451  0.70196078  0.77254902]\n",
      "  [ 0.80784314  0.39607843  0.52941176]\n",
      "  [ 0.14509804  0.38039216  0.91372549]\n",
      "  ..., \n",
      "  [ 0.84313725  0.75686275  0.27058824]\n",
      "  [ 0.43921569  0.31764706  0.62745098]\n",
      "  [ 0.08235294  0.38823529  0.38039216]]\n",
      "\n",
      " ..., \n",
      " [[ 0.00392157  0.34901961  0.91372549]\n",
      "  [ 0.30196078  0.25490196  0.49411765]\n",
      "  [ 0.34509804  0.10588235  0.18431373]\n",
      "  ..., \n",
      "  [ 0.54117647  0.21960784  0.51372549]\n",
      "  [ 0.84313725  0.99607843  0.49803922]\n",
      "  [ 0.71764706  0.05882353  0.08627451]]\n",
      "\n",
      " [[ 0.89411765  0.69411765  0.83137255]\n",
      "  [ 0.4627451   0.52156863  0.08235294]\n",
      "  [ 0.69803922  0.20392157  0.3254902 ]\n",
      "  ..., \n",
      "  [ 0.65490196  0.42745098  0.10588235]\n",
      "  [ 0.65882353  0.61176471  0.98431373]\n",
      "  [ 0.45882353  0.68627451  0.29411765]]\n",
      "\n",
      " [[ 0.1372549   0.36078431  0.51372549]\n",
      "  [ 0.00784314  0.02352941  0.49411765]\n",
      "  [ 0.88235294  0.30980392  0.0627451 ]\n",
      "  ..., \n",
      "  [ 0.43529412  0.30980392  0.62352941]\n",
      "  [ 0.89411765  0.34509804  0.49803922]\n",
      "  [ 0.11764706  0.22745098  0.78823529]]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print(x[1])\n",
    "    print(len(x))\n",
    "    x_min = x.min(axis=(0, 1), keepdims=True)\n",
    "    x_max = x.max(axis=(0, 1), keepdims=True)\n",
    "    x = (x - x_min)/(x_max - x_min)\n",
    "    print(x[1])\n",
    "    return x\n",
    "\n",
    "    # ALTERNATIVE: return x / np.max(x)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[0 0 0 0 0 0 0 1 0 0]\n",
      "6\n",
      "[0 0 0 0 0 0 1 0 0 0]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print(x[1])\n",
    "    label = preprocessing.LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)    \n",
    "    label.fit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    one_hot_label = label.transform(x)\n",
    "    print(one_hot_label[1])\n",
    "    return one_hot_label\n",
    "\n",
    "    # ALTERNATIVE return np.eye(10)[x]\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[154 177 187]\n",
      "  [126 137 136]\n",
      "  [105 104  95]\n",
      "  ..., \n",
      "  [ 91  95  71]\n",
      "  [ 87  90  71]\n",
      "  [ 79  81  70]]\n",
      "\n",
      " [[140 160 169]\n",
      "  [145 153 154]\n",
      "  [125 125 118]\n",
      "  ..., \n",
      "  [ 96  99  78]\n",
      "  [ 77  80  62]\n",
      "  [ 71  73  61]]\n",
      "\n",
      " [[140 155 164]\n",
      "  [139 146 149]\n",
      "  [115 115 112]\n",
      "  ..., \n",
      "  [ 79  82  64]\n",
      "  [ 68  70  55]\n",
      "  [ 67  69  55]]\n",
      "\n",
      " ..., \n",
      " [[175 167 166]\n",
      "  [156 154 160]\n",
      "  [154 160 170]\n",
      "  ..., \n",
      "  [ 42  34  36]\n",
      "  [ 61  53  57]\n",
      "  [ 93  83  91]]\n",
      "\n",
      " [[165 154 128]\n",
      "  [156 152 130]\n",
      "  [159 161 142]\n",
      "  ..., \n",
      "  [103  93  96]\n",
      "  [123 114 120]\n",
      "  [131 121 131]]\n",
      "\n",
      " [[163 148 120]\n",
      "  [158 148 122]\n",
      "  [163 156 133]\n",
      "  ..., \n",
      "  [143 133 139]\n",
      "  [143 134 142]\n",
      "  [143 133 144]]]\n",
      "9000\n",
      "[[[ 0.60392157  0.69411765  0.73333333]\n",
      "  [ 0.49411765  0.5372549   0.53333333]\n",
      "  [ 0.41176471  0.40784314  0.37254902]\n",
      "  ..., \n",
      "  [ 0.35686275  0.37254902  0.27843137]\n",
      "  [ 0.34117647  0.35294118  0.27843137]\n",
      "  [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      " [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "  [ 0.56862745  0.6         0.60392157]\n",
      "  [ 0.49019608  0.49019608  0.4627451 ]\n",
      "  ..., \n",
      "  [ 0.37647059  0.38823529  0.30588235]\n",
      "  [ 0.30196078  0.31372549  0.24313725]\n",
      "  [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      " [[ 0.54901961  0.60784314  0.64313725]\n",
      "  [ 0.54509804  0.57254902  0.58431373]\n",
      "  [ 0.45098039  0.45098039  0.43921569]\n",
      "  ..., \n",
      "  [ 0.30980392  0.32156863  0.25098039]\n",
      "  [ 0.26666667  0.2745098   0.21568627]\n",
      "  [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      " ..., \n",
      " [[ 0.68627451  0.65490196  0.65098039]\n",
      "  [ 0.61176471  0.60392157  0.62745098]\n",
      "  [ 0.60392157  0.62745098  0.66666667]\n",
      "  ..., \n",
      "  [ 0.16470588  0.13333333  0.14117647]\n",
      "  [ 0.23921569  0.20784314  0.22352941]\n",
      "  [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      " [[ 0.64705882  0.60392157  0.50196078]\n",
      "  [ 0.61176471  0.59607843  0.50980392]\n",
      "  [ 0.62352941  0.63137255  0.55686275]\n",
      "  ..., \n",
      "  [ 0.40392157  0.36470588  0.37647059]\n",
      "  [ 0.48235294  0.44705882  0.47058824]\n",
      "  [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      " [[ 0.63921569  0.58039216  0.47058824]\n",
      "  [ 0.61960784  0.58039216  0.47843137]\n",
      "  [ 0.63921569  0.61176471  0.52156863]\n",
      "  ..., \n",
      "  [ 0.56078431  0.52156863  0.54509804]\n",
      "  [ 0.56078431  0.5254902   0.55686275]\n",
      "  [ 0.56078431  0.52156863  0.56470588]]]\n",
      "9\n",
      "[0 0 0 0 0 0 0 0 0 1]\n",
      "[[[ 20  15  12]\n",
      "  [ 20  15  12]\n",
      "  [ 18  13  10]\n",
      "  ..., \n",
      "  [ 20  16  16]\n",
      "  [ 21  16  14]\n",
      "  [ 21  16  13]]\n",
      "\n",
      " [[ 20  15  12]\n",
      "  [ 20  15  12]\n",
      "  [ 18  13  10]\n",
      "  ..., \n",
      "  [ 20  16  15]\n",
      "  [ 21  16  13]\n",
      "  [ 21  16  13]]\n",
      "\n",
      " [[ 20  15  12]\n",
      "  [ 20  15  12]\n",
      "  [ 18  13  10]\n",
      "  ..., \n",
      "  [ 20  16  14]\n",
      "  [ 21  16  13]\n",
      "  [ 21  16  13]]\n",
      "\n",
      " ..., \n",
      " [[ 66  54  41]\n",
      "  [ 80  67  53]\n",
      "  [ 47  35  19]\n",
      "  ..., \n",
      "  [134 134 100]\n",
      "  [110 113  77]\n",
      "  [ 98 102  66]]\n",
      "\n",
      " [[ 60  48  33]\n",
      "  [ 55  43  27]\n",
      "  [ 50  38  22]\n",
      "  ..., \n",
      "  [123 125  83]\n",
      "  [ 79  81  42]\n",
      "  [ 72  74  38]]\n",
      "\n",
      " [[ 64  54  38]\n",
      "  [ 55  45  29]\n",
      "  [ 48  38  21]\n",
      "  ..., \n",
      "  [155 157 111]\n",
      "  [136 137  97]\n",
      "  [ 88  89  51]]]\n",
      "9000\n",
      "[[[ 0.07843137  0.05882353  0.04705882]\n",
      "  [ 0.07843137  0.05882353  0.04705882]\n",
      "  [ 0.07058824  0.05098039  0.03921569]\n",
      "  ..., \n",
      "  [ 0.07843137  0.0627451   0.0627451 ]\n",
      "  [ 0.08235294  0.0627451   0.05490196]\n",
      "  [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      " [[ 0.07843137  0.05882353  0.04705882]\n",
      "  [ 0.07843137  0.05882353  0.04705882]\n",
      "  [ 0.07058824  0.05098039  0.03921569]\n",
      "  ..., \n",
      "  [ 0.07843137  0.0627451   0.05882353]\n",
      "  [ 0.08235294  0.0627451   0.05098039]\n",
      "  [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      " [[ 0.07843137  0.05882353  0.04705882]\n",
      "  [ 0.07843137  0.05882353  0.04705882]\n",
      "  [ 0.07058824  0.05098039  0.03921569]\n",
      "  ..., \n",
      "  [ 0.07843137  0.0627451   0.05490196]\n",
      "  [ 0.08235294  0.0627451   0.05098039]\n",
      "  [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      " ..., \n",
      " [[ 0.25882353  0.21176471  0.16078431]\n",
      "  [ 0.31372549  0.2627451   0.20784314]\n",
      "  [ 0.18431373  0.1372549   0.0745098 ]\n",
      "  ..., \n",
      "  [ 0.5254902   0.5254902   0.39215686]\n",
      "  [ 0.43137255  0.44313725  0.30196078]\n",
      "  [ 0.38431373  0.4         0.25882353]]\n",
      "\n",
      " [[ 0.23529412  0.18823529  0.12941176]\n",
      "  [ 0.21568627  0.16862745  0.10588235]\n",
      "  [ 0.19607843  0.14901961  0.08627451]\n",
      "  ..., \n",
      "  [ 0.48235294  0.49019608  0.3254902 ]\n",
      "  [ 0.30980392  0.31764706  0.16470588]\n",
      "  [ 0.28235294  0.29019608  0.14901961]]\n",
      "\n",
      " [[ 0.25098039  0.21176471  0.14901961]\n",
      "  [ 0.21568627  0.17647059  0.11372549]\n",
      "  [ 0.18823529  0.14901961  0.08235294]\n",
      "  ..., \n",
      "  [ 0.60784314  0.61568627  0.43529412]\n",
      "  [ 0.53333333  0.5372549   0.38039216]\n",
      "  [ 0.34509804  0.34901961  0.2       ]]]\n",
      "6\n",
      "[0 0 0 0 0 0 1 0 0 0]\n",
      "[[[ 94  86  58]\n",
      "  [101  91  61]\n",
      "  [ 95  85  54]\n",
      "  ..., \n",
      "  [145 138 106]\n",
      "  [145 140 108]\n",
      "  [121 117  90]]\n",
      "\n",
      " [[ 89  84  55]\n",
      "  [ 97  89  59]\n",
      "  [101  91  60]\n",
      "  ..., \n",
      "  [146 137 106]\n",
      "  [146 139 107]\n",
      "  [122 117  90]]\n",
      "\n",
      " [[ 86  84  54]\n",
      "  [ 94  88  57]\n",
      "  [107  98  67]\n",
      "  ..., \n",
      "  [146 136 106]\n",
      "  [147 138 107]\n",
      "  [123 116  90]]\n",
      "\n",
      " ..., \n",
      " [[205 205 204]\n",
      "  [208 208 207]\n",
      "  [201 201 200]\n",
      "  ..., \n",
      "  [145 149 149]\n",
      "  [149 152 157]\n",
      "  [125 128 135]]\n",
      "\n",
      " [[201 201 201]\n",
      "  [205 205 205]\n",
      "  [198 198 198]\n",
      "  ..., \n",
      "  [154 158 169]\n",
      "  [158 162 177]\n",
      "  [134 138 153]]\n",
      "\n",
      " [[190 190 189]\n",
      "  [188 188 187]\n",
      "  [175 175 173]\n",
      "  ..., \n",
      "  [162 164 182]\n",
      "  [163 166 184]\n",
      "  [135 137 155]]]\n",
      "9000\n",
      "[[[ 0.36862745  0.3372549   0.22745098]\n",
      "  [ 0.39607843  0.35686275  0.23921569]\n",
      "  [ 0.37254902  0.33333333  0.21176471]\n",
      "  ..., \n",
      "  [ 0.56862745  0.54117647  0.41568627]\n",
      "  [ 0.56862745  0.54901961  0.42352941]\n",
      "  [ 0.4745098   0.45882353  0.35294118]]\n",
      "\n",
      " [[ 0.34901961  0.32941176  0.21568627]\n",
      "  [ 0.38039216  0.34901961  0.23137255]\n",
      "  [ 0.39607843  0.35686275  0.23529412]\n",
      "  ..., \n",
      "  [ 0.57254902  0.5372549   0.41568627]\n",
      "  [ 0.57254902  0.54509804  0.41960784]\n",
      "  [ 0.47843137  0.45882353  0.35294118]]\n",
      "\n",
      " [[ 0.3372549   0.32941176  0.21176471]\n",
      "  [ 0.36862745  0.34509804  0.22352941]\n",
      "  [ 0.41960784  0.38431373  0.2627451 ]\n",
      "  ..., \n",
      "  [ 0.57254902  0.53333333  0.41568627]\n",
      "  [ 0.57647059  0.54117647  0.41960784]\n",
      "  [ 0.48235294  0.45490196  0.35294118]]\n",
      "\n",
      " ..., \n",
      " [[ 0.80392157  0.80392157  0.8       ]\n",
      "  [ 0.81568627  0.81568627  0.81176471]\n",
      "  [ 0.78823529  0.78823529  0.78431373]\n",
      "  ..., \n",
      "  [ 0.56862745  0.58431373  0.58431373]\n",
      "  [ 0.58431373  0.59607843  0.61568627]\n",
      "  [ 0.49019608  0.50196078  0.52941176]]\n",
      "\n",
      " [[ 0.78823529  0.78823529  0.78823529]\n",
      "  [ 0.80392157  0.80392157  0.80392157]\n",
      "  [ 0.77647059  0.77647059  0.77647059]\n",
      "  ..., \n",
      "  [ 0.60392157  0.61960784  0.6627451 ]\n",
      "  [ 0.61960784  0.63529412  0.69411765]\n",
      "  [ 0.5254902   0.54117647  0.6       ]]\n",
      "\n",
      " [[ 0.74509804  0.74509804  0.74117647]\n",
      "  [ 0.7372549   0.7372549   0.73333333]\n",
      "  [ 0.68627451  0.68627451  0.67843137]\n",
      "  ..., \n",
      "  [ 0.63529412  0.64313725  0.71372549]\n",
      "  [ 0.63921569  0.65098039  0.72156863]\n",
      "  [ 0.52941176  0.5372549   0.60784314]]]\n",
      "5\n",
      "[0 0 0 0 0 1 0 0 0 0]\n",
      "[[[ 29  43  10]\n",
      "  [ 22  36   4]\n",
      "  [ 25  37  16]\n",
      "  ..., \n",
      "  [197 219 137]\n",
      "  [198 219 137]\n",
      "  [199 222 140]]\n",
      "\n",
      " [[ 31  46   9]\n",
      "  [ 27  41   6]\n",
      "  [ 17  29   6]\n",
      "  ..., \n",
      "  [210 232 148]\n",
      "  [209 231 148]\n",
      "  [209 231 148]]\n",
      "\n",
      " [[ 40  55  16]\n",
      "  [ 31  45   8]\n",
      "  [ 20  33   7]\n",
      "  ..., \n",
      "  [210 232 150]\n",
      "  [210 232 149]\n",
      "  [210 232 149]]\n",
      "\n",
      " ..., \n",
      " [[ 45  38  23]\n",
      "  [ 24  21  11]\n",
      "  [ 16  14   7]\n",
      "  ..., \n",
      "  [ 25  29  32]\n",
      "  [ 24  28  31]\n",
      "  [ 24  28  31]]\n",
      "\n",
      " [[ 21  18   7]\n",
      "  [ 18  13   3]\n",
      "  [ 27  16   5]\n",
      "  ..., \n",
      "  [ 26  30  33]\n",
      "  [ 29  33  36]\n",
      "  [ 28  32  35]]\n",
      "\n",
      " [[ 53  40  23]\n",
      "  [ 81  62  38]\n",
      "  [ 97  70  43]\n",
      "  ..., \n",
      "  [ 22  26  29]\n",
      "  [ 24  28  31]\n",
      "  [ 23  27  30]]]\n",
      "9000\n",
      "[[[ 0.11372549  0.16862745  0.03921569]\n",
      "  [ 0.08627451  0.14117647  0.01568627]\n",
      "  [ 0.09803922  0.14509804  0.0627451 ]\n",
      "  ..., \n",
      "  [ 0.77254902  0.85882353  0.5372549 ]\n",
      "  [ 0.77647059  0.85882353  0.5372549 ]\n",
      "  [ 0.78039216  0.87058824  0.54901961]]\n",
      "\n",
      " [[ 0.12156863  0.18039216  0.03529412]\n",
      "  [ 0.10588235  0.16078431  0.02352941]\n",
      "  [ 0.06666667  0.11372549  0.02352941]\n",
      "  ..., \n",
      "  [ 0.82352941  0.90980392  0.58039216]\n",
      "  [ 0.81960784  0.90588235  0.58039216]\n",
      "  [ 0.81960784  0.90588235  0.58039216]]\n",
      "\n",
      " [[ 0.15686275  0.21568627  0.0627451 ]\n",
      "  [ 0.12156863  0.17647059  0.03137255]\n",
      "  [ 0.07843137  0.12941176  0.02745098]\n",
      "  ..., \n",
      "  [ 0.82352941  0.90980392  0.58823529]\n",
      "  [ 0.82352941  0.90980392  0.58431373]\n",
      "  [ 0.82352941  0.90980392  0.58431373]]\n",
      "\n",
      " ..., \n",
      " [[ 0.17647059  0.14901961  0.09019608]\n",
      "  [ 0.09411765  0.08235294  0.04313725]\n",
      "  [ 0.0627451   0.05490196  0.02745098]\n",
      "  ..., \n",
      "  [ 0.09803922  0.11372549  0.1254902 ]\n",
      "  [ 0.09411765  0.10980392  0.12156863]\n",
      "  [ 0.09411765  0.10980392  0.12156863]]\n",
      "\n",
      " [[ 0.08235294  0.07058824  0.02745098]\n",
      "  [ 0.07058824  0.05098039  0.01176471]\n",
      "  [ 0.10588235  0.0627451   0.01960784]\n",
      "  ..., \n",
      "  [ 0.10196078  0.11764706  0.12941176]\n",
      "  [ 0.11372549  0.12941176  0.14117647]\n",
      "  [ 0.10980392  0.1254902   0.1372549 ]]\n",
      "\n",
      " [[ 0.20784314  0.15686275  0.09019608]\n",
      "  [ 0.31764706  0.24313725  0.14901961]\n",
      "  [ 0.38039216  0.2745098   0.16862745]\n",
      "  ..., \n",
      "  [ 0.08627451  0.10196078  0.11372549]\n",
      "  [ 0.09411765  0.10980392  0.12156863]\n",
      "  [ 0.09019608  0.10588235  0.11764706]]]\n",
      "6\n",
      "[0 0 0 0 0 0 1 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[127 145 167]\n",
      "  [126 144 166]\n",
      "  [127 145 167]\n",
      "  ..., \n",
      "  [125 142 159]\n",
      "  [125 142 159]\n",
      "  [124 141 158]]\n",
      "\n",
      " [[125 143 164]\n",
      "  [125 143 163]\n",
      "  [126 144 164]\n",
      "  ..., \n",
      "  [125 142 157]\n",
      "  [124 142 156]\n",
      "  [124 141 156]]\n",
      "\n",
      " [[126 145 162]\n",
      "  [124 143 160]\n",
      "  [126 145 162]\n",
      "  ..., \n",
      "  [124 143 156]\n",
      "  [123 141 155]\n",
      "  [123 141 155]]\n",
      "\n",
      " ..., \n",
      " [[ 84 104 118]\n",
      "  [ 85 104 118]\n",
      "  [ 87 106 120]\n",
      "  ..., \n",
      "  [ 65  83  95]\n",
      "  [ 79  97 109]\n",
      "  [ 88 106 118]]\n",
      "\n",
      " [[ 86 105 120]\n",
      "  [ 83 102 116]\n",
      "  [ 83 102 116]\n",
      "  ..., \n",
      "  [ 73  91 103]\n",
      "  [ 83 101 113]\n",
      "  [ 87 105 117]]\n",
      "\n",
      " [[ 85 104 118]\n",
      "  [ 85 103 117]\n",
      "  [ 83 102 116]\n",
      "  ..., \n",
      "  [ 72  90 102]\n",
      "  [ 78  96 108]\n",
      "  [ 82 100 112]]]\n",
      "9000\n",
      "[[[ 0.49803922  0.56862745  0.65490196]\n",
      "  [ 0.49411765  0.56470588  0.65098039]\n",
      "  [ 0.49803922  0.56862745  0.65490196]\n",
      "  ..., \n",
      "  [ 0.49019608  0.55686275  0.62352941]\n",
      "  [ 0.49019608  0.55686275  0.62352941]\n",
      "  [ 0.48627451  0.55294118  0.61960784]]\n",
      "\n",
      " [[ 0.49019608  0.56078431  0.64313725]\n",
      "  [ 0.49019608  0.56078431  0.63921569]\n",
      "  [ 0.49411765  0.56470588  0.64313725]\n",
      "  ..., \n",
      "  [ 0.49019608  0.55686275  0.61568627]\n",
      "  [ 0.48627451  0.55686275  0.61176471]\n",
      "  [ 0.48627451  0.55294118  0.61176471]]\n",
      "\n",
      " [[ 0.49411765  0.56862745  0.63529412]\n",
      "  [ 0.48627451  0.56078431  0.62745098]\n",
      "  [ 0.49411765  0.56862745  0.63529412]\n",
      "  ..., \n",
      "  [ 0.48627451  0.56078431  0.61176471]\n",
      "  [ 0.48235294  0.55294118  0.60784314]\n",
      "  [ 0.48235294  0.55294118  0.60784314]]\n",
      "\n",
      " ..., \n",
      " [[ 0.32941176  0.40784314  0.4627451 ]\n",
      "  [ 0.33333333  0.40784314  0.4627451 ]\n",
      "  [ 0.34117647  0.41568627  0.47058824]\n",
      "  ..., \n",
      "  [ 0.25490196  0.3254902   0.37254902]\n",
      "  [ 0.30980392  0.38039216  0.42745098]\n",
      "  [ 0.34509804  0.41568627  0.4627451 ]]\n",
      "\n",
      " [[ 0.3372549   0.41176471  0.47058824]\n",
      "  [ 0.3254902   0.4         0.45490196]\n",
      "  [ 0.3254902   0.4         0.45490196]\n",
      "  ..., \n",
      "  [ 0.28627451  0.35686275  0.40392157]\n",
      "  [ 0.3254902   0.39607843  0.44313725]\n",
      "  [ 0.34117647  0.41176471  0.45882353]]\n",
      "\n",
      " [[ 0.33333333  0.40784314  0.4627451 ]\n",
      "  [ 0.33333333  0.40392157  0.45882353]\n",
      "  [ 0.3254902   0.4         0.45490196]\n",
      "  ..., \n",
      "  [ 0.28235294  0.35294118  0.4       ]\n",
      "  [ 0.30588235  0.37647059  0.42352941]\n",
      "  [ 0.32156863  0.39215686  0.43921569]]]\n",
      "8\n",
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[[[100 109  84]\n",
      "  [122 126 109]\n",
      "  [ 87  87  76]\n",
      "  ..., \n",
      "  [ 75  78  69]\n",
      "  [ 70  73  64]\n",
      "  [ 70  73  64]]\n",
      "\n",
      " [[ 86  99  71]\n",
      "  [ 76  84  66]\n",
      "  [ 60  64  54]\n",
      "  ..., \n",
      "  [ 78  81  72]\n",
      "  [ 76  79  70]\n",
      "  [ 82  85  76]]\n",
      "\n",
      " [[ 84 100  73]\n",
      "  [ 83  95  75]\n",
      "  [ 77  86  72]\n",
      "  ..., \n",
      "  [ 74  77  68]\n",
      "  [ 73  76  67]\n",
      "  [ 83  86  77]]\n",
      "\n",
      " ..., \n",
      " [[ 64  77  79]\n",
      "  [122 133 144]\n",
      "  [134 145 156]\n",
      "  ..., \n",
      "  [105 123 120]\n",
      "  [ 84 103  91]\n",
      "  [ 60  88  63]]\n",
      "\n",
      " [[ 44  51  56]\n",
      "  [ 78  84  94]\n",
      "  [ 96 101 110]\n",
      "  ..., \n",
      "  [147 165 178]\n",
      "  [126 143 149]\n",
      "  [ 94 117 113]]\n",
      "\n",
      " [[ 36  35  39]\n",
      "  [ 59  58  66]\n",
      "  [ 82  81  85]\n",
      "  ..., \n",
      "  [134 153 160]\n",
      "  [138 152 158]\n",
      "  [130 148 150]]]\n",
      "5000\n",
      "[[[ 0.39215686  0.42745098  0.32941176]\n",
      "  [ 0.47843137  0.49411765  0.42745098]\n",
      "  [ 0.34117647  0.34117647  0.29803922]\n",
      "  ..., \n",
      "  [ 0.29411765  0.30588235  0.27058824]\n",
      "  [ 0.2745098   0.28627451  0.25098039]\n",
      "  [ 0.2745098   0.28627451  0.25098039]]\n",
      "\n",
      " [[ 0.3372549   0.38823529  0.27843137]\n",
      "  [ 0.29803922  0.32941176  0.25882353]\n",
      "  [ 0.23529412  0.25098039  0.21176471]\n",
      "  ..., \n",
      "  [ 0.30588235  0.31764706  0.28235294]\n",
      "  [ 0.29803922  0.30980392  0.2745098 ]\n",
      "  [ 0.32156863  0.33333333  0.29803922]]\n",
      "\n",
      " [[ 0.32941176  0.39215686  0.28627451]\n",
      "  [ 0.3254902   0.37254902  0.29411765]\n",
      "  [ 0.30196078  0.3372549   0.28235294]\n",
      "  ..., \n",
      "  [ 0.29019608  0.30196078  0.26666667]\n",
      "  [ 0.28627451  0.29803922  0.2627451 ]\n",
      "  [ 0.3254902   0.3372549   0.30196078]]\n",
      "\n",
      " ..., \n",
      " [[ 0.25098039  0.30196078  0.30980392]\n",
      "  [ 0.47843137  0.52156863  0.56470588]\n",
      "  [ 0.5254902   0.56862745  0.61176471]\n",
      "  ..., \n",
      "  [ 0.41176471  0.48235294  0.47058824]\n",
      "  [ 0.32941176  0.40392157  0.35686275]\n",
      "  [ 0.23529412  0.34509804  0.24705882]]\n",
      "\n",
      " [[ 0.17254902  0.2         0.21960784]\n",
      "  [ 0.30588235  0.32941176  0.36862745]\n",
      "  [ 0.37647059  0.39607843  0.43137255]\n",
      "  ..., \n",
      "  [ 0.57647059  0.64705882  0.69803922]\n",
      "  [ 0.49411765  0.56078431  0.58431373]\n",
      "  [ 0.36862745  0.45882353  0.44313725]]\n",
      "\n",
      " [[ 0.14117647  0.1372549   0.15294118]\n",
      "  [ 0.23137255  0.22745098  0.25882353]\n",
      "  [ 0.32156863  0.31764706  0.33333333]\n",
      "  ..., \n",
      "  [ 0.5254902   0.6         0.62745098]\n",
      "  [ 0.54117647  0.59607843  0.61960784]\n",
      "  [ 0.50980392  0.58039216  0.58823529]]]\n",
      "2\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "[[[235 235 235]\n",
      "  [231 231 231]\n",
      "  [232 232 232]\n",
      "  ..., \n",
      "  [233 233 233]\n",
      "  [233 233 233]\n",
      "  [232 232 232]]\n",
      "\n",
      " [[238 238 238]\n",
      "  [235 235 235]\n",
      "  [235 235 235]\n",
      "  ..., \n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [235 235 235]]\n",
      "\n",
      " [[237 237 237]\n",
      "  [234 234 234]\n",
      "  [234 234 234]\n",
      "  ..., \n",
      "  [235 235 235]\n",
      "  [235 235 235]\n",
      "  [234 234 234]]\n",
      "\n",
      " ..., \n",
      " [[ 87  99  89]\n",
      "  [ 43  51  37]\n",
      "  [ 19  23  11]\n",
      "  ..., \n",
      "  [169 184 179]\n",
      "  [182 197 193]\n",
      "  [188 202 201]]\n",
      "\n",
      " [[ 82  96  82]\n",
      "  [ 46  57  36]\n",
      "  [ 36  44  22]\n",
      "  ..., \n",
      "  [174 189 183]\n",
      "  [185 200 196]\n",
      "  [187 202 200]]\n",
      "\n",
      " [[ 85 101  83]\n",
      "  [ 62  75  48]\n",
      "  [ 58  67  38]\n",
      "  ..., \n",
      "  [168 183 178]\n",
      "  [180 195 191]\n",
      "  [186 200 199]]]\n",
      "10000\n",
      "[[[ 0.92156863  0.92156863  0.92156863]\n",
      "  [ 0.90588235  0.90588235  0.90588235]\n",
      "  [ 0.90980392  0.90980392  0.90980392]\n",
      "  ..., \n",
      "  [ 0.91372549  0.91372549  0.91372549]\n",
      "  [ 0.91372549  0.91372549  0.91372549]\n",
      "  [ 0.90980392  0.90980392  0.90980392]]\n",
      "\n",
      " [[ 0.93333333  0.93333333  0.93333333]\n",
      "  [ 0.92156863  0.92156863  0.92156863]\n",
      "  [ 0.92156863  0.92156863  0.92156863]\n",
      "  ..., \n",
      "  [ 0.9254902   0.9254902   0.9254902 ]\n",
      "  [ 0.9254902   0.9254902   0.9254902 ]\n",
      "  [ 0.92156863  0.92156863  0.92156863]]\n",
      "\n",
      " [[ 0.92941176  0.92941176  0.92941176]\n",
      "  [ 0.91764706  0.91764706  0.91764706]\n",
      "  [ 0.91764706  0.91764706  0.91764706]\n",
      "  ..., \n",
      "  [ 0.92156863  0.92156863  0.92156863]\n",
      "  [ 0.92156863  0.92156863  0.92156863]\n",
      "  [ 0.91764706  0.91764706  0.91764706]]\n",
      "\n",
      " ..., \n",
      " [[ 0.34117647  0.38823529  0.34901961]\n",
      "  [ 0.16862745  0.2         0.14509804]\n",
      "  [ 0.0745098   0.09019608  0.04313725]\n",
      "  ..., \n",
      "  [ 0.6627451   0.72156863  0.70196078]\n",
      "  [ 0.71372549  0.77254902  0.75686275]\n",
      "  [ 0.7372549   0.79215686  0.78823529]]\n",
      "\n",
      " [[ 0.32156863  0.37647059  0.32156863]\n",
      "  [ 0.18039216  0.22352941  0.14117647]\n",
      "  [ 0.14117647  0.17254902  0.08627451]\n",
      "  ..., \n",
      "  [ 0.68235294  0.74117647  0.71764706]\n",
      "  [ 0.7254902   0.78431373  0.76862745]\n",
      "  [ 0.73333333  0.79215686  0.78431373]]\n",
      "\n",
      " [[ 0.33333333  0.39607843  0.3254902 ]\n",
      "  [ 0.24313725  0.29411765  0.18823529]\n",
      "  [ 0.22745098  0.2627451   0.14901961]\n",
      "  ..., \n",
      "  [ 0.65882353  0.71764706  0.69803922]\n",
      "  [ 0.70588235  0.76470588  0.74901961]\n",
      "  [ 0.72941176  0.78431373  0.78039216]]]\n",
      "8\n",
      "[0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(4, 4)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    depth = int(x_tensor.shape[3])\n",
    "    image_height, image_width = conv_ksize\n",
    "    print(conv_ksize)\n",
    "    print(conv_strides)\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal([image_height, image_width, depth, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs, activation=None, use_bias=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(1, 1)\n",
      "(3, 3)\n",
      "(1, 1)\n",
      "(3, 3)\n",
      "(1, 1)\n",
      "(3, 3)\n",
      "(1, 1)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = conv2d_maxpool(x, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x = fully_conn(x, 128)\n",
    "    x = tf.nn.dropout(x, keep_prob)   \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    x = output(x, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    print('Loss: ', loss, ' Accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "keep_probability = 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  2.01706  Accuracy:  0.3524\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  1.75713  Accuracy:  0.4266\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  1.51599  Accuracy:  0.4732\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  1.32221  Accuracy:  0.504\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  1.2609  Accuracy:  0.5102\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  1.11011  Accuracy:  0.513\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  0.94656  Accuracy:  0.5226\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  0.852666  Accuracy:  0.5386\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  0.802958  Accuracy:  0.5438\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  0.729496  Accuracy:  0.5414\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  0.667638  Accuracy:  0.551\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  0.659574  Accuracy:  0.5554\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  0.578537  Accuracy:  0.573\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  0.505453  Accuracy:  0.5756\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  0.462184  Accuracy:  0.578\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  0.430298  Accuracy:  0.5834\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  0.386487  Accuracy:  0.5876\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  0.337284  Accuracy:  0.5904\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  0.346583  Accuracy:  0.586\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  0.280145  Accuracy:  0.5878\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  0.293494  Accuracy:  0.5748\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  0.237655  Accuracy:  0.5844\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  0.231465  Accuracy:  0.5688\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  0.230731  Accuracy:  0.5696\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  0.261493  Accuracy:  0.5604\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  0.201603  Accuracy:  0.5768\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  0.19065  Accuracy:  0.573\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  0.160529  Accuracy:  0.5988\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  0.14189  Accuracy:  0.5914\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  0.130152  Accuracy:  0.5992\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  0.154347  Accuracy:  0.5742\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  0.118849  Accuracy:  0.5916\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  0.113693  Accuracy:  0.6022\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  0.0893954  Accuracy:  0.5986\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  0.115624  Accuracy:  0.5852\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  0.100535  Accuracy:  0.6024\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  0.0870216  Accuracy:  0.6018\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  0.0775075  Accuracy:  0.6044\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  0.0729994  Accuracy:  0.5966\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  0.0784766  Accuracy:  0.5946\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  0.0563574  Accuracy:  0.5954\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  0.0469586  Accuracy:  0.5946\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  0.0451447  Accuracy:  0.6044\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  0.0470549  Accuracy:  0.6018\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  0.0332698  Accuracy:  0.5998\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  0.0328303  Accuracy:  0.612\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  0.029429  Accuracy:  0.606\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  0.0364503  Accuracy:  0.601\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  0.0336049  Accuracy:  0.6086\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  0.026746  Accuracy:  0.6036\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  1.96583  Accuracy:  0.3674\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:  1.65136  Accuracy:  0.431\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:  1.35171  Accuracy:  0.4548\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:  1.45716  Accuracy:  0.4766\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:  1.41003  Accuracy:  0.4996\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  1.3792  Accuracy:  0.5134\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:  1.2186  Accuracy:  0.529\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:  0.990861  Accuracy:  0.5362\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:  1.11904  Accuracy:  0.567\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:  1.1534  Accuracy:  0.5618\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  1.1638  Accuracy:  0.584\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:  0.944918  Accuracy:  0.5826\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:  0.808696  Accuracy:  0.5904\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:  0.983121  Accuracy:  0.5974\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:  0.944421  Accuracy:  0.601\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  1.00655  Accuracy:  0.6176\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:  0.773167  Accuracy:  0.6108\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:  0.642823  Accuracy:  0.6112\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:  0.821131  Accuracy:  0.629\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:  0.815157  Accuracy:  0.6156\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  0.821751  Accuracy:  0.6282\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:  0.657146  Accuracy:  0.635\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:  0.518116  Accuracy:  0.6446\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:  0.734127  Accuracy:  0.6432\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:  0.681316  Accuracy:  0.6258\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  0.68006  Accuracy:  0.6338\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:  0.529252  Accuracy:  0.6526\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:  0.453539  Accuracy:  0.6524\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:  0.647598  Accuracy:  0.6476\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:  0.582539  Accuracy:  0.6432\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  0.572838  Accuracy:  0.647\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:  0.4426  Accuracy:  0.6594\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:  0.429916  Accuracy:  0.6458\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:  0.572665  Accuracy:  0.6634\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:  0.528029  Accuracy:  0.6378\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  0.503457  Accuracy:  0.6536\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:  0.403083  Accuracy:  0.661\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:  0.365468  Accuracy:  0.6626\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:  0.493152  Accuracy:  0.6636\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:  0.439983  Accuracy:  0.6676\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  0.454056  Accuracy:  0.654\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:  0.306726  Accuracy:  0.6688\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:  0.334997  Accuracy:  0.661\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:  0.427771  Accuracy:  0.68\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:  0.367622  Accuracy:  0.6634\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  0.367096  Accuracy:  0.6688\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:  0.279254  Accuracy:  0.692\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:  0.274937  Accuracy:  0.6764\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:  0.388802  Accuracy:  0.6826\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:  0.323359  Accuracy:  0.6788\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  0.344941  Accuracy:  0.671\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:  0.239646  Accuracy:  0.687\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:  0.234951  Accuracy:  0.682\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:  0.341468  Accuracy:  0.6934\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:  0.285546  Accuracy:  0.687\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  0.32761  Accuracy:  0.6824\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:  0.206217  Accuracy:  0.6924\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:  0.201094  Accuracy:  0.685\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:  0.314032  Accuracy:  0.695\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:  0.250206  Accuracy:  0.6942\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  0.267727  Accuracy:  0.6862\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:  0.187831  Accuracy:  0.695\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:  0.184998  Accuracy:  0.6794\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:  0.282137  Accuracy:  0.6996\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:  0.218291  Accuracy:  0.698\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  0.254367  Accuracy:  0.6814\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:  0.163696  Accuracy:  0.6998\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:  0.174147  Accuracy:  0.678\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:  0.243193  Accuracy:  0.6978\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:  0.206951  Accuracy:  0.6996\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  0.222727  Accuracy:  0.6884\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:  0.16532  Accuracy:  0.682\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:  0.149281  Accuracy:  0.6864\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:  0.21601  Accuracy:  0.7062\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:  0.170422  Accuracy:  0.7046\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  0.199702  Accuracy:  0.7026\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:  0.134144  Accuracy:  0.6998\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:  0.134448  Accuracy:  0.685\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:  0.195156  Accuracy:  0.7026\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:  0.154478  Accuracy:  0.7038\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  0.213898  Accuracy:  0.6946\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:  0.127696  Accuracy:  0.689\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:  0.106793  Accuracy:  0.6984\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:  0.17099  Accuracy:  0.7066\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:  0.139569  Accuracy:  0.7028\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  0.203767  Accuracy:  0.7016\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:  0.117887  Accuracy:  0.692\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:  0.11588  Accuracy:  0.6956\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:  0.150103  Accuracy:  0.6902\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:  0.133107  Accuracy:  0.7098\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  0.175488  Accuracy:  0.7094\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:  0.104486  Accuracy:  0.6824\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:  0.0940078  Accuracy:  0.7096\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:  0.127767  Accuracy:  0.696\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:  0.134406  Accuracy:  0.7074\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  0.126169  Accuracy:  0.7112\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:  0.0962249  Accuracy:  0.6806\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:  0.0790343  Accuracy:  0.71\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:  0.129015  Accuracy:  0.6966\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:  0.116555  Accuracy:  0.7054\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  0.103913  Accuracy:  0.6984\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:  0.0788548  Accuracy:  0.6872\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:  0.0704832  Accuracy:  0.7074\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:  0.11935  Accuracy:  0.6984\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:  0.0968903  Accuracy:  0.6954\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  0.112534  Accuracy:  0.7014\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:  0.0780793  Accuracy:  0.6912\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:  0.0840796  Accuracy:  0.704\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:  0.0988807  Accuracy:  0.705\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:  0.0914233  Accuracy:  0.7004\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  0.0931076  Accuracy:  0.7068\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:  0.062777  Accuracy:  0.6906\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:  0.0694556  Accuracy:  0.7072\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:  0.082489  Accuracy:  0.7084\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:  0.0614081  Accuracy:  0.706\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  0.101189  Accuracy:  0.6974\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:  0.0688162  Accuracy:  0.6834\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:  0.0704909  Accuracy:  0.7044\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:  0.090362  Accuracy:  0.6968\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:  0.0678488  Accuracy:  0.701\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  0.0993779  Accuracy:  0.6938\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:  0.0619171  Accuracy:  0.6772\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:  0.0491707  Accuracy:  0.7054\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:  0.0831961  Accuracy:  0.6944\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:  0.0648296  Accuracy:  0.7068\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  0.0624436  Accuracy:  0.696\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:  0.0611567  Accuracy:  0.6882\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:  0.0529647  Accuracy:  0.7014\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:  0.0698846  Accuracy:  0.6976\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:  0.0497008  Accuracy:  0.7078\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  0.0573293  Accuracy:  0.6958\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:  0.0515254  Accuracy:  0.6784\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:  0.058361  Accuracy:  0.7024\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:  0.0672879  Accuracy:  0.706\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:  0.0377791  Accuracy:  0.701\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  0.0618288  Accuracy:  0.7012\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:  0.0403358  Accuracy:  0.6936\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:  0.0452442  Accuracy:  0.7002\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:  0.0633633  Accuracy:  0.706\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:  0.0400633  Accuracy:  0.7014\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  0.0625492  Accuracy:  0.7034\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:  0.0340009  Accuracy:  0.693\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:  0.0485658  Accuracy:  0.6948\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:  0.0766982  Accuracy:  0.6974\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:  0.0391244  Accuracy:  0.6958\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  0.0473612  Accuracy:  0.7004\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:  0.0302993  Accuracy:  0.6908\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:  0.0359823  Accuracy:  0.7038\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:  0.0533502  Accuracy:  0.704\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:  0.0379167  Accuracy:  0.7016\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  0.0354285  Accuracy:  0.7048\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:  0.0282217  Accuracy:  0.6956\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:  0.0365264  Accuracy:  0.7058\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:  0.0397133  Accuracy:  0.7028\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:  0.0325947  Accuracy:  0.6998\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  0.0301746  Accuracy:  0.7034\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:  0.0278567  Accuracy:  0.6994\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:  0.0348744  Accuracy:  0.7056\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:  0.0418855  Accuracy:  0.693\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:  0.0340381  Accuracy:  0.706\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  0.0225237  Accuracy:  0.7084\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:  0.0221168  Accuracy:  0.6986\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:  0.0315026  Accuracy:  0.705\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:  0.0389257  Accuracy:  0.6962\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:  0.0311767  Accuracy:  0.7092\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  0.0278342  Accuracy:  0.7092\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:  0.0186577  Accuracy:  0.6904\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:  0.0283848  Accuracy:  0.6958\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:  0.0287589  Accuracy:  0.7056\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:  0.0352167  Accuracy:  0.7028\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  0.029149  Accuracy:  0.704\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:  0.0263556  Accuracy:  0.6854\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:  0.0224618  Accuracy:  0.705\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:  0.0252491  Accuracy:  0.707\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:  0.0249596  Accuracy:  0.6912\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  0.0530384  Accuracy:  0.6998\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:  0.0286551  Accuracy:  0.6992\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:  0.0204802  Accuracy:  0.697\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:  0.0238075  Accuracy:  0.7092\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:  0.0321972  Accuracy:  0.7002\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  0.0318746  Accuracy:  0.7062\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:  0.022152  Accuracy:  0.6918\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:  0.0150938  Accuracy:  0.7034\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:  0.0276492  Accuracy:  0.7038\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:  0.0223791  Accuracy:  0.6916\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  0.0242768  Accuracy:  0.6982\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:  0.0268879  Accuracy:  0.6922\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:  0.01717  Accuracy:  0.7104\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:  0.0318453  Accuracy:  0.7002\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:  0.0230085  Accuracy:  0.6926\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  0.0243241  Accuracy:  0.6904\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:  0.0203071  Accuracy:  0.6982\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:  0.0115811  Accuracy:  0.6956\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:  0.0338587  Accuracy:  0.705\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:  0.0176427  Accuracy:  0.7004\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  0.0272959  Accuracy:  0.6998\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:  0.0165887  Accuracy:  0.7032\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:  0.0141022  Accuracy:  0.6964\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:  0.0289555  Accuracy:  0.6984\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:  0.0195692  Accuracy:  0.7006\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  0.030852  Accuracy:  0.6956\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:  0.0116052  Accuracy:  0.698\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:  0.00952655  Accuracy:  0.6926\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:  0.0187067  Accuracy:  0.7002\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:  0.0136831  Accuracy:  0.7042\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  0.023935  Accuracy:  0.6888\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:  0.0153828  Accuracy:  0.7046\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:  0.0158292  Accuracy:  0.6908\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:  0.019945  Accuracy:  0.7032\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:  0.0155895  Accuracy:  0.696\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  0.0210338  Accuracy:  0.6994\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:  0.0171588  Accuracy:  0.7066\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:  0.0141477  Accuracy:  0.6976\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:  0.0218199  Accuracy:  0.6958\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:  0.0129159  Accuracy:  0.694\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  0.0176522  Accuracy:  0.6966\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:  0.0131333  Accuracy:  0.6986\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:  0.0134209  Accuracy:  0.6924\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:  0.0152141  Accuracy:  0.6916\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:  0.00939885  Accuracy:  0.7034\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  0.0199905  Accuracy:  0.6982\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:  0.0162614  Accuracy:  0.6994\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:  0.00847068  Accuracy:  0.6872\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:  0.0137375  Accuracy:  0.6976\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:  0.00755767  Accuracy:  0.7018\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  0.024427  Accuracy:  0.6924\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:  0.011108  Accuracy:  0.6996\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:  0.00541234  Accuracy:  0.6904\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:  0.0152727  Accuracy:  0.6926\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:  0.00674985  Accuracy:  0.7002\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  0.0181195  Accuracy:  0.6966\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:  0.00781416  Accuracy:  0.6988\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:  0.00539517  Accuracy:  0.6902\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:  0.0168189  Accuracy:  0.6828\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:  0.00900084  Accuracy:  0.692\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  0.0200365  Accuracy:  0.694\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:  0.0127657  Accuracy:  0.6928\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:  0.00613735  Accuracy:  0.7004\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:  0.0187121  Accuracy:  0.6808\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:  0.00775691  Accuracy:  0.701\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  0.0177611  Accuracy:  0.6904\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:  0.0124415  Accuracy:  0.6954\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:  0.00456627  Accuracy:  0.7008\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:  0.0140126  Accuracy:  0.683\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:  0.00576958  Accuracy:  0.6916\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  0.0114237  Accuracy:  0.6908\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:  0.00388825  Accuracy:  0.7016\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:  0.00422542  Accuracy:  0.6972\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:  0.0118621  Accuracy:  0.684\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:  0.00561639  Accuracy:  0.6934\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.68349609375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86b6a6f128>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
